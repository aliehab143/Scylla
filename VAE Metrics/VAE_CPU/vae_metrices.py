# -*- coding: utf-8 -*-
"""VAE_Metrices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FSPXeljtW-qPzVJ6Hi3pOmMxnvLM-RX0
"""

import pandas as pd
import numpy as np
import json
import urllib.request
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import os

# --- Data Loading and Preprocessing ---

# Load CPU usage data from NAB
data_url = "https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/cpu_utilization_asg_misconfiguration.csv"
df = pd.read_csv(data_url)
df['timestamp'] = pd.to_datetime(df['timestamp'])
df.set_index('timestamp', inplace=True)

# Normalize the data (z-score)
mean = df['value'].mean()
std = df['value'].std()
df['value'] = (df['value'] - mean) / std

# Split into training (70%) and test sets
train_size = int(len(df) * 0.7)
train_df = df.iloc[:train_size]
test_df = df.iloc[train_size:]
print(f"Original training data size: {len(train_df)}, Test data size: {len(test_df)}")

# Load NAB anomaly labels
labels_url = "https://raw.githubusercontent.com/numenta/NAB/master/labels/combined_windows.json"
with urllib.request.urlopen(labels_url) as url:
    labels = json.load(url)
anomaly_windows = labels['realKnownCause/cpu_utilization_asg_misconfiguration.csv']
anomaly_windows = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in anomaly_windows]

# Filter anomalies from training data
train_df['anomaly'] = 0
for start, end in anomaly_windows:
    train_df.loc[(train_df.index >= start) & (train_df.index <= end), 'anomaly'] = 1
train_df_normal = train_df[train_df['anomaly'] == 0]
print(f"Training data size after filtering anomalies: {len(train_df_normal)}")

test_df['anomaly'] = 0
for start, end in anomaly_windows:
    test_df.loc[(test_df.index >= start) & (test_df.index <= end), 'anomaly'] = 1
print(f"Number of anomalous points in test data: {test_df['anomaly'].sum()}")

# Create sequences
seq_length = 100

def to_sequences(data, seq_length):
    sequences = []
    for i in range(len(data) - seq_length):
        sequences.append(data[i:i + seq_length])
    return np.array(sequences)

train_sequences = to_sequences(train_df_normal['value'].values, seq_length)
test_sequences = to_sequences(test_df['value'].values, seq_length)

# Reshape sequences to (samples, seq_length, 1)
train_sequences = train_sequences.reshape((train_sequences.shape[0], seq_length, 1))
test_sequences = test_sequences.reshape((test_sequences.shape[0], seq_length, 1))

# --- Build VAE Architecture ---

latent_dim = 128  # Dimension of the latent space

# Encoder
def build_encoder(input_shape):
    encoder_inputs = keras.Input(shape=input_shape)
    x = layers.Conv1D(32, 7, padding='same', activation='relu')(encoder_inputs)
    x = layers.MaxPooling1D(2)(x)
    x = layers.Conv1D(16, 7, padding='same', activation='relu')(x)
    x = layers.MaxPooling1D(2)(x)
    x = layers.Flatten()(x)
    z_mean = layers.Dense(latent_dim, name='z_mean')(x)
    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)
    return keras.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')

# Sampling layer
def sampling(args):
    z_mean, z_log_var = args
    epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# Decoder
def build_decoder(output_shape):
    latent_inputs = keras.Input(shape=(latent_dim,))
    x = layers.Dense((output_shape[0] // 4) * 16, activation='relu')(latent_inputs)
    x = layers.Reshape(((output_shape[0] // 4), 16))(x)
    x = layers.Conv1DTranspose(16, 7, padding='same', activation='relu')(x)
    x = layers.UpSampling1D(2)(x)
    x = layers.Conv1DTranspose(32, 7, padding='same', activation='relu')(x)
    x = layers.UpSampling1D(2)(x)
    outputs = layers.Conv1D(1, 7, padding='same', activation='linear')(x)
    return keras.Model(latent_inputs, outputs, name='decoder')

# Custom VAE Model
class VAE(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.total_loss_tracker = keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = keras.metrics.Mean(name="reconstruction_loss")
        self.kl_loss_tracker = keras.metrics.Mean(name="kl_loss")

    def call(self, inputs, training=False):
        z_mean, z_log_var = self.encoder(inputs)
        z = sampling([z_mean, z_log_var])
        reconstruction = self.decoder(z)
        return reconstruction

    def train_step(self, data):
        if isinstance(data, tuple):
            data = data[0]
        with tf.GradientTape() as tape:
            z_mean, z_log_var = self.encoder(data)
            z = sampling([z_mean, z_log_var])
            reconstruction = self.decoder(z)
            reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(tf.square(data - reconstruction), axis=[1, 2])
            )
            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
            )
            total_loss = reconstruction_loss + kl_loss
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
        }

    def test_step(self, data):
        if isinstance(data, tuple):
            data = data[0]
        z_mean, z_log_var = self.encoder(data)
        z = sampling([z_mean, z_log_var])
        reconstruction = self.decoder(z)
        reconstruction_loss = tf.reduce_mean(
            tf.reduce_sum(tf.square(data - reconstruction), axis=[1, 2])
        )
        kl_loss = -0.5 * tf.reduce_mean(
            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
        )
        total_loss = reconstruction_loss + kl_loss
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
        }

    def get_config(self):
        config = super(VAE, self).get_config()
        config.update({
            'encoder_config': self.encoder.get_config(),
            'decoder_config': self.decoder.get_config()
        })
        return config

    @classmethod
    def from_config(cls, config, custom_objects=None):
        encoder = keras.models.Model.from_config(config['encoder_config'], custom_objects)
        decoder = keras.models.Model.from_config(config['decoder_config'], custom_objects)
        return cls(encoder=encoder, decoder=decoder)

# Instantiate models
input_shape = (seq_length, 1)
encoder = build_encoder(input_shape)
decoder = build_decoder(input_shape)
vae = VAE(encoder, decoder)

# Compile VAE
vae.compile(optimizer=keras.optimizers.Adam())

# --- Train VAE ---
vae.fit(
    train_sequences,
    epochs=50,
    batch_size=128,
    validation_split=0.1,
    callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

# --- Compute Reconstruction Errors and Set Threshold ---

# Encode and decode training data to compute reconstruction error
reconstructed_train = vae.predict(train_sequences)
train_mae = np.mean(np.abs(reconstructed_train - train_sequences), axis=(1, 2))
threshold = np.mean(train_mae) + 2 * np.std(train_mae)

# Encode and decode test data
reconstructed_test = vae.predict(test_sequences)
test_mae = np.mean(np.abs(reconstructed_test - test_sequences), axis=(1, 2))

# Detect anomalies
anomalies = test_mae > threshold

# --- Save Model and Settings ---

# Directory to save model and settings
save_dir = "vae_anomaly_detection"
os.makedirs(save_dir, exist_ok=True)

# Save the VAE model with .keras extension
model_save_path = os.path.join(save_dir, "vae_model.keras")
vae.save(model_save_path)
print(f"Model saved to: {model_save_path}")

# Save settings to a JSON file
settings = {
    "threshold": float(threshold),
    "seq_length": int(seq_length),
    "mean": float(mean),
    "std": float(std)
}

settings_save_path = os.path.join(save_dir, "settings.json")
with open(settings_save_path, 'w') as f:
    json.dump(settings, f, indent=4)
print(f"Settings saved to: {settings_save_path}")

# --- Plot Test Data, Detected Anomalies, and Real Anomalies ---

plt.figure(figsize=(15, 6))
plt.plot(test_df.index[seq_length:], test_df['value'].values[seq_length:], label='Test Data', zorder=1)
plt.plot(test_df.index[seq_length:], test_mae, label='Reconstruction Error', alpha=0.5, zorder=2)
plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold', zorder=3)

# Plot detected anomalies (red circles)
anomaly_indices = np.where(anomalies)[0]
anomaly_times = test_df.index[seq_length:][anomaly_indices]
anomaly_values = test_df['value'].values[seq_length:][anomaly_indices]
plt.scatter(anomaly_times, anomaly_values, color='red', label='Detected Anomalies', marker='o', s=50, alpha=0.7, zorder=4)

# Plot real anomalies (green triangles)
real_anomaly_indices = np.where(test_df['anomaly'].values[seq_length:])[0]
real_anomaly_times = test_df.index[seq_length:][real_anomaly_indices]
real_anomaly_values = test_df['value'].values[seq_length:][real_anomaly_indices]
plt.scatter(real_anomaly_times, real_anomaly_values, color='green', label='Real Anomalies', marker='^', s=50, alpha=0.7, zorder=5)

plt.legend()
plt.title('Anomaly Detection with VAE')
plt.xlabel('Timestamp')
plt.ylabel('Value / Error')
plt.savefig(os.path.join(save_dir, 'anomaly_detection_plot.png'))
plt.close()

# --- Print Threshold and Metrics ---

print(f"Anomaly detection threshold: {threshold:.4f}")
print(f"Number of anomalies detected in test data: {np.sum(anomalies)}")

# Align ground truth labels with predicted anomalies
true_labels = test_df['anomaly'].values[seq_length:]

# Ensure lengths match
if len(true_labels) != len(anomalies):
    min_length = min(len(true_labels), len(anomalies))
    true_labels = true_labels[:min_length]
    anomalies = anomalies[:min_length]

# Calculate total anomalies (ground truth)
total_anomalies = np.sum(true_labels)
print(f"Total number of anomalies in ground truth: {total_anomalies}")

# Compute confusion matrix
tn, fp, fn, tp = confusion_matrix(true_labels, anomalies).ravel()

# Print results
print(f"Correctly predicted anomalies (True Positives): {tp}")
print(f"Incorrectly predicted anomalies:")
print(f"  - False Positives (normal predicted as anomaly): {fp}")
print(f"  - False Negatives (anomaly predicted as normal): {fn}")

# Additional metrics
accuracy = accuracy_score(true_labels, anomalies)
precision = precision_score(true_labels, anomalies, zero_division=0)
recall = recall_score(true_labels, anomalies, zero_division=0)
print(f"\nAdditional Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
